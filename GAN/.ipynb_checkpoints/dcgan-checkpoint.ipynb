{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1; Elapsed time = 0s\n",
      "10/11: G=1.632, Dr=0.769, Df=0.364\n",
      "loss_g_running 1.9313878655433654\n",
      "loss_d_real_running0.8366838932037354\n",
      "loss_d_fake_running0.37464807108044623\n",
      "Epoch 2; Elapsed time = 3s\n",
      "10/11: G=4.632, Dr=0.284, Df=0.039\n",
      "loss_g_running 5.356492733955383\n",
      "loss_d_real_running0.2887645110487938\n",
      "loss_d_fake_running0.03993834345601499\n",
      "Epoch 3; Elapsed time = 6s\n",
      "10/11: G=7.896, Dr=0.017, Df=0.004\n",
      "loss_g_running 8.821696567535401\n",
      "loss_d_real_running0.017529046209529043\n",
      "loss_d_fake_running0.003970673750154674\n",
      "Epoch 4; Elapsed time = 9s\n",
      "10/11: G=9.483, Dr=0.005, Df=0.001\n",
      "loss_g_running 10.678926563262939\n",
      "loss_d_real_running0.005157414695713669\n",
      "loss_d_fake_running0.0012984418543055653\n",
      "Epoch 5; Elapsed time = 12s\n",
      "10/11: G=11.319, Dr=0.003, Df=0.001\n",
      "loss_g_running 12.509157085418702\n",
      "loss_d_real_running0.0035250726155936717\n",
      "loss_d_fake_running0.001029639522312209\n",
      "Epoch 6; Elapsed time = 16s\n",
      "10/11: G=12.126, Dr=0.002, Df=0.000\n",
      "loss_g_running 13.313544273376465\n",
      "loss_d_real_running0.002367155803949572\n",
      "loss_d_fake_running0.0005467010703796404\n",
      "Epoch 7; Elapsed time = 19s\n",
      "10/11: G=13.180, Dr=0.001, Df=0.001\n",
      "loss_g_running 14.646388053894043\n",
      "loss_d_real_running0.0012801545788533985\n",
      "loss_d_fake_running0.0007991936450252978\n",
      "Epoch 8; Elapsed time = 22s\n",
      "10/11: G=14.176, Dr=0.001, Df=0.000\n",
      "loss_g_running 15.648303127288818\n",
      "loss_d_real_running0.0010828389204107225\n",
      "loss_d_fake_running0.0001989117783978145\n",
      "Epoch 9; Elapsed time = 26s\n",
      "10/11: G=14.923, Dr=0.001, Df=0.000\n",
      "loss_g_running 16.38466567993164\n",
      "loss_d_real_running0.000753967312630266\n",
      "loss_d_fake_running6.810084589687903e-05\n",
      "Epoch 10; Elapsed time = 29s\n",
      "10/11: G=14.587, Dr=0.000, Df=0.000\n",
      "loss_g_running 15.931321239471435\n",
      "loss_d_real_running0.0005004005535738543\n",
      "loss_d_fake_running6.918933795532211e-05\n",
      "92\n",
      "[{'label': 0, 'x': 0, 'y': 1, 'orientation': 2}, {'label': 0, 'x': 3, 'y': 1, 'orientation': 3}, {'label': 0, 'x': 8, 'y': 1, 'orientation': 3}, {'label': 0, 'x': 0, 'y': 3, 'orientation': 1}, {'label': 0, 'x': 1, 'y': 3, 'orientation': 1}, {'label': 0, 'x': 4, 'y': 5, 'orientation': 3}, {'label': 0, 'x': 1, 'y': 7, 'orientation': 3}, {'label': 0, 'x': 2, 'y': 7, 'orientation': 3}, {'label': 0, 'x': 4, 'y': 8, 'orientation': 3}, {'label': 0, 'x': 7, 'y': 9, 'orientation': 2}, {'label': 0, 'x': 0, 'y': 11, 'orientation': 0}, {'label': 0, 'x': 4, 'y': 11, 'orientation': 0}, {'label': 1, 'x': 6, 'y': 3, 'orientation': 0}, {'label': 1, 'x': 8, 'y': 3, 'orientation': 3}, {'label': 1, 'x': 9, 'y': 4, 'orientation': 0}, {'label': 1, 'x': 2, 'y': 6, 'orientation': 3}, {'label': 1, 'x': 8, 'y': 6, 'orientation': 1}, {'label': 1, 'x': 8, 'y': 8, 'orientation': 1}, {'label': 1, 'x': 8, 'y': 9, 'orientation': 1}, {'label': 1, 'x': 3, 'y': 10, 'orientation': 3}, {'label': 2, 'x': 5, 'y': 2, 'orientation': 3}, {'label': 2, 'x': 9, 'y': 2, 'orientation': 0}, {'label': 2, 'x': 7, 'y': 4, 'orientation': 3}, {'label': 3, 'x': 0, 'y': 5, 'orientation': 0}, {'label': 3, 'x': 0, 'y': 9, 'orientation': 2}, {'label': 3, 'x': 2, 'y': 9, 'orientation': 0}, {'label': 3, 'x': 4, 'y': 9, 'orientation': 3}, {'label': 3, 'x': 8, 'y': 11, 'orientation': 3}, {'label': 4, 'x': 1, 'y': 6, 'orientation': 3}, {'label': 4, 'x': 3, 'y': 11, 'orientation': 2}, {'label': 5, 'x': 2, 'y': 1, 'orientation': 2}, {'label': 5, 'x': 6, 'y': 1, 'orientation': 3}, {'label': 5, 'x': 1, 'y': 2, 'orientation': 3}, {'label': 5, 'x': 1, 'y': 8, 'orientation': 3}, {'label': 5, 'x': 6, 'y': 8, 'orientation': 3}, {'label': 5, 'x': 1, 'y': 12, 'orientation': 0}, {'label': 6, 'x': 7, 'y': 2, 'orientation': 3}, {'label': 6, 'x': 7, 'y': 3, 'orientation': 2}, {'label': 6, 'x': 3, 'y': 8, 'orientation': 3}, {'label': 7, 'x': 5, 'y': 6, 'orientation': 0}, {'label': 7, 'x': 7, 'y': 6, 'orientation': 1}, {'label': 7, 'x': 9, 'y': 6, 'orientation': 3}, {'label': 7, 'x': 0, 'y': 7, 'orientation': 0}, {'label': 7, 'x': 2, 'y': 8, 'orientation': 2}, {'label': 7, 'x': 6, 'y': 9, 'orientation': 3}, {'label': 7, 'x': 7, 'y': 10, 'orientation': 3}, {'label': 8, 'x': 3, 'y': 0, 'orientation': 3}, {'label': 8, 'x': 8, 'y': 2, 'orientation': 1}, {'label': 8, 'x': 6, 'y': 6, 'orientation': 3}, {'label': 8, 'x': 4, 'y': 10, 'orientation': 3}, {'label': 8, 'x': 8, 'y': 10, 'orientation': 3}, {'label': 8, 'x': 2, 'y': 12, 'orientation': 3}, {'label': 8, 'x': 6, 'y': 12, 'orientation': 2}, {'label': 9, 'x': 7, 'y': 5, 'orientation': 2}, {'label': 9, 'x': 4, 'y': 6, 'orientation': 2}, {'label': 9, 'x': 6, 'y': 7, 'orientation': 3}, {'label': 9, 'x': 9, 'y': 9, 'orientation': 1}, {'label': 9, 'x': 6, 'y': 11, 'orientation': 3}, {'label': 10, 'x': 2, 'y': 3, 'orientation': 0}, {'label': 10, 'x': 4, 'y': 3, 'orientation': 0}, {'label': 10, 'x': 9, 'y': 5, 'orientation': 3}, {'label': 10, 'x': 8, 'y': 7, 'orientation': 3}, {'label': 11, 'x': 4, 'y': 2, 'orientation': 2}, {'label': 11, 'x': 6, 'y': 2, 'orientation': 2}, {'label': 11, 'x': 3, 'y': 3, 'orientation': 2}, {'label': 11, 'x': 1, 'y': 5, 'orientation': 1}, {'label': 11, 'x': 3, 'y': 5, 'orientation': 2}, {'label': 11, 'x': 5, 'y': 5, 'orientation': 1}, {'label': 11, 'x': 3, 'y': 7, 'orientation': 1}, {'label': 11, 'x': 5, 'y': 7, 'orientation': 2}, {'label': 11, 'x': 5, 'y': 9, 'orientation': 3}, {'label': 11, 'x': 5, 'y': 11, 'orientation': 2}, {'label': 11, 'x': 7, 'y': 11, 'orientation': 2}, {'label': 12, 'x': 9, 'y': 3, 'orientation': 2}, {'label': 12, 'x': 1, 'y': 4, 'orientation': 3}, {'label': 12, 'x': 3, 'y': 4, 'orientation': 3}, {'label': 12, 'x': 7, 'y': 7, 'orientation': 3}, {'label': 12, 'x': 7, 'y': 8, 'orientation': 3}, {'label': 12, 'x': 9, 'y': 8, 'orientation': 3}, {'label': 12, 'x': 3, 'y': 9, 'orientation': 3}, {'label': 12, 'x': 1, 'y': 10, 'orientation': 3}, {'label': 12, 'x': 1, 'y': 11, 'orientation': 2}, {'label': 12, 'x': 9, 'y': 11, 'orientation': 2}, {'label': 13, 'x': 6, 'y': 4, 'orientation': 3}, {'label': 14, 'x': 4, 'y': 1, 'orientation': 1}, {'label': 14, 'x': 9, 'y': 1, 'orientation': 2}, {'label': 14, 'x': 2, 'y': 5, 'orientation': 0}, {'label': 14, 'x': 6, 'y': 5, 'orientation': 3}, {'label': 14, 'x': 8, 'y': 5, 'orientation': 3}, {'label': 14, 'x': 4, 'y': 7, 'orientation': 3}, {'label': 14, 'x': 9, 'y': 7, 'orientation': 2}, {'label': 14, 'x': 2, 'y': 10, 'orientation': 0}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision as tv\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=10, batchnorm=True):\n",
    "        \"\"\"A generator for mapping a latent space to a sample space.\n",
    "        The sample space for this generator is single-channel, 28x28 images\n",
    "        with pixel intensity ranging from -1 to +1.\n",
    "        Args:\n",
    "            latent_dim (int): latent dimension (\"noise vector\")\n",
    "            batchnorm (bool): Whether or not to use batch normalization\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batchnorm = batchnorm\n",
    "        self._init_modules()\n",
    "\n",
    "    def _init_modules(self):\n",
    "        \"\"\"Initialize the modules.\"\"\"\n",
    "        # Project the input\n",
    "        self.linear1 = nn.Linear(self.latent_dim, 272*6*5, bias=False)\n",
    "        self.bn1d1 = nn.BatchNorm1d(272*6*5) if self.batchnorm else None\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "        # Convolutions\n",
    "        self.conv1 = nn.Conv2d(\n",
    "                in_channels=272,\n",
    "                out_channels=136,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False)\n",
    "        #6X5\n",
    "        self.bn2d1 = nn.BatchNorm2d(136) if self.batchnorm else None\n",
    "\n",
    "        self.conv2 = nn.ConvTranspose2d(\n",
    "                in_channels=136,\n",
    "                out_channels=68,\n",
    "                kernel_size=2,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=False)\n",
    "        #7X6\n",
    "        self.bn2d2 = nn.BatchNorm2d(68) if self.batchnorm else None\n",
    "\n",
    "        self.conv3 = nn.ConvTranspose2d(\n",
    "                in_channels=68,\n",
    "                out_channels=17,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False)\n",
    "        #13X11\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
    "        intermediate = self.linear1(input_tensor)\n",
    "        intermediate = self.bn1d1(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "\n",
    "        intermediate = intermediate.view((-1, 272, 6, 5))\n",
    "\n",
    "        intermediate = self.conv1(intermediate)\n",
    "        if self.batchnorm:\n",
    "            intermediate = self.bn2d1(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "\n",
    "        intermediate = self.conv2(intermediate)\n",
    "        \n",
    "        #print(intermediate.size())\n",
    "        if self.batchnorm:\n",
    "            intermediate = self.bn2d2(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "\n",
    "        intermediate = self.conv3(intermediate)\n",
    "        \n",
    "        #print(intermediate.size())\n",
    "        intermediate = intermediate.narrow(3, 0, 10)\n",
    "        output_tensor = intermediate\n",
    "        #output_tensor = self.tanh(intermediate)\n",
    "        \n",
    "\n",
    "        new_tensor = torch.zeros(output_tensor.size())\n",
    "\n",
    "        for i, batch in enumerate(output_tensor):\n",
    "            max_indices = torch.argmax(batch.narrow(0, 0, 15), dim=0)\n",
    "            for j in range(max_indices.size(dim=0)):\n",
    "                for k in range(max_indices.size(dim=1)):\n",
    "                    if batch[max_indices[j][k]][j][k] > 0.5: \n",
    "                        new_tensor[i][max_indices[j][k]][j][k] = 1\n",
    "                        if batch[15][j][k].abs() > batch[16][j][k].abs():\n",
    "                            new_tensor[i][15][j][k] = 1 if batch[15][j][k] > 0 else -1\n",
    "                        else:\n",
    "                            new_tensor[i][16][j][k] = 1 if batch[16][j][k] > 0 else -1\n",
    "\n",
    "        \n",
    "        return new_tensor\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"A discriminator for discerning real from generated images.\n",
    "        Images must be single-channel and 28x28 pixels.\n",
    "        Output activation is Sigmoid.\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        self._init_modules()\n",
    "\n",
    "    def _init_modules(self):\n",
    "        \"\"\"Initialize the modules.\"\"\"\n",
    "        self.conv1 = nn.Conv2d(\n",
    "                in_channels=17,\n",
    "                out_channels=68,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=2,\n",
    "                bias=False)\n",
    "        #7X6\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.dropout_2d = nn.Dropout2d(0.3)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "                in_channels=68,\n",
    "                out_channels=136,\n",
    "                kernel_size=2,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=False)\n",
    "        #6X5\n",
    "        self.linear1 = nn.Linear(136*6*5, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map samples to confidence they are real [0, 1].\"\"\"\n",
    "        intermediate = self.conv1(input_tensor)\n",
    "        \n",
    "        #print(intermediate.size())\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.dropout_2d(intermediate)\n",
    "\n",
    "        intermediate = self.conv2(intermediate)\n",
    "        \n",
    "        #print(intermediate.size())\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.dropout_2d(intermediate)\n",
    "\n",
    "        intermediate = intermediate.view((-1, 136*6*5))\n",
    "        intermediate = self.linear1(intermediate)\n",
    "        output_tensor = self.sigmoid(intermediate)\n",
    "        #print(\"in dis forward\")\n",
    "        #print(output_tensor.size())\n",
    "        #print(output_tensor)\n",
    "        #print(\"finished\")\n",
    "        return output_tensor\n",
    "\n",
    "class DCGAN():\n",
    "    def __init__(self, latent_dim, noise_fn, dataloader,\n",
    "                 batch_size=32, device='cpu', lr_d=1e-3, lr_g=2e-4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            generator: a Ganerator network\n",
    "            discriminator: A Discriminator network\n",
    "            noise_fn: function f(num: int) -> pytorch tensor, (latent vectors)\n",
    "            dataloader: a pytorch dataloader for loading images\n",
    "            batch_size: training batch size. Must match that of dataloader\n",
    "            device: cpu or CUDA\n",
    "            lr_d: learning rate for the discriminator\n",
    "            lr_g: learning rate for the generator\n",
    "        \"\"\"\n",
    "        self.generator = Generator(latent_dim).to(device)\n",
    "        self.discriminator = Discriminator().to(device)\n",
    "        self.noise_fn = noise_fn\n",
    "        self.dataloader = dataloader\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optim_d = optim.Adam(self.discriminator.parameters(),\n",
    "                                  lr=lr_d, betas=(0.5, 0.999))\n",
    "        self.optim_g = optim.Adam(self.generator.parameters(),\n",
    "                                  lr=lr_g, betas=(0.5, 0.999))\n",
    "        self.target_ones = torch.ones((batch_size, 1), device=device)\n",
    "        self.target_zeros = torch.zeros((batch_size, 1), device=device)\n",
    "\n",
    "    def generate_samples(self, latent_vec=None, num=None):\n",
    "        \"\"\"Sample images from the generator.\n",
    "        Images are returned as a 4D tensor of values between -1 and 1.\n",
    "        Dimensions are (number, channels, height, width). Returns the tensor\n",
    "        on cpu.\n",
    "        Args:\n",
    "            latent_vec: A pytorch latent vector or None\n",
    "            num: The number of samples to generate if latent_vec is None\n",
    "        If latent_vec and num are None then use self.batch_size\n",
    "        random latent vectors.\n",
    "        \"\"\"\n",
    "        num = self.batch_size if num is None else num\n",
    "        latent_vec = self.noise_fn(num) if latent_vec is None else latent_vec\n",
    "        with torch.no_grad():\n",
    "            samples = self.generator(latent_vec)\n",
    "        samples = samples.cpu()  # move images to cpu\n",
    "        return samples\n",
    "\n",
    "    def train_step_generator(self):\n",
    "        \"\"\"Train the generator one step and return the loss.\"\"\"\n",
    "        self.generator.zero_grad()\n",
    "\n",
    "        latent_vec = self.noise_fn(self.batch_size)\n",
    "        generated = self.generator(latent_vec)\n",
    "        classifications = self.discriminator(generated)\n",
    "        loss = self.criterion(classifications, self.target_ones)\n",
    "        loss.backward()\n",
    "        self.optim_g.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train_step_discriminator(self, real_samples):\n",
    "        \"\"\"Train the discriminator one step and return the losses.\"\"\"\n",
    "        self.discriminator.zero_grad()\n",
    "\n",
    "        # real samples\n",
    "        pred_real = self.discriminator(real_samples)\n",
    "        #print(\"size of pre_real\")\n",
    "        #print(pred_real.size())\n",
    "        #print(\"size of ones\")\n",
    "        #print(self.target_ones.size())\n",
    "        loss_real = self.criterion(pred_real, self.target_ones)\n",
    "\n",
    "        # generated samples\n",
    "        latent_vec = self.noise_fn(self.batch_size)\n",
    "        with torch.no_grad():\n",
    "            fake_samples = self.generator(latent_vec)\n",
    "        pred_fake = self.discriminator(fake_samples)\n",
    "        loss_fake = self.criterion(pred_fake, self.target_zeros)\n",
    "\n",
    "        # combine\n",
    "        loss = (loss_real + loss_fake) / 2\n",
    "        loss.backward()\n",
    "        self.optim_d.step()\n",
    "        return loss_real.item(), loss_fake.item()\n",
    "\n",
    "    def train_epoch(self, print_frequency=10, max_steps=0):\n",
    "        \"\"\"Train both networks for one epoch and return the losses.\n",
    "        Args:\n",
    "            print_frequency (int): print stats every `print_frequency` steps.\n",
    "            max_steps (int): End epoch after `max_steps` steps, or set to 0\n",
    "                             to do the full epoch.\n",
    "        \"\"\"\n",
    "        loss_g_running, loss_d_real_running, loss_d_fake_running = 0, 0, 0\n",
    "        for batch, (real_samples) in enumerate(self.dataloader):\n",
    "            #print(\"in train_epoch\")\n",
    "            #print(batch)\n",
    "            #print(real_samples.size())\n",
    "            #print()\n",
    "            real_samples = real_samples.to(self.device)\n",
    "            ldr_, ldf_ = self.train_step_discriminator(real_samples)\n",
    "            loss_d_real_running += ldr_\n",
    "            loss_d_fake_running += ldf_\n",
    "            loss_g_running += self.train_step_generator()\n",
    "            if print_frequency and (batch+1) % print_frequency == 0:\n",
    "                print(f\"{batch+1}/{len(self.dataloader)}:\"\n",
    "                      f\" G={loss_g_running / (batch+1):.3f},\"\n",
    "                      f\" Dr={loss_d_real_running / (batch+1):.3f},\"\n",
    "                      f\" Df={loss_d_fake_running / (batch+1):.3f}\",\n",
    "                      end='\\r',\n",
    "                      flush=True)\n",
    "            if max_steps and batch == max_steps:\n",
    "                break\n",
    "        if print_frequency:\n",
    "            print()\n",
    "        loss_g_running /= batch\n",
    "        loss_d_real_running /= batch\n",
    "        loss_d_fake_running /= batch\n",
    "        print(\"loss_g_running \" + str(loss_g_running))\n",
    "        print(\"loss_d_real_running\" + str(loss_d_real_running))\n",
    "        print(\"loss_d_fake_running\" + str(loss_d_fake_running))\n",
    "        return (loss_g_running, (loss_d_real_running, loss_d_fake_running))\n",
    "    \n",
    "    def trans_samples(self, samples):\n",
    "        room_list = []\n",
    "        obj_list = []\n",
    "        obj_dict = {\"label\":0,\n",
    "                     \"x\":0,\n",
    "                     \"y\":0,\n",
    "                     \"orientation\":0}\n",
    "        for num in range(samples.size(dim=0)):\n",
    "            for i in range(samples.size(dim=1)-2):\n",
    "                for j in range(samples.size(dim=2)):\n",
    "                    for k in range(samples.size(dim=3)):\n",
    "                        if samples[num][i][j][k] == 1:\n",
    "                            obj_dict[\"label\"] = i\n",
    "                            obj_dict[\"y\"] = j\n",
    "                            obj_dict[\"x\"] = k\n",
    "                            if samples[num][16][j][k] == -1:\n",
    "                                obj_dict[\"orientation\"] = 0\n",
    "                            elif samples[num][16][j][k] == 1:\n",
    "                                obj_dict[\"orientation\"] = 2\n",
    "                            elif samples[num][15][j][k] == -1:\n",
    "                                obj_dict[\"orientation\"] = 1\n",
    "                            else:\n",
    "                                obj_dict[\"orientation\"] = 3\n",
    "                            #tmp = obj_dict.copy()\n",
    "                            obj_list.append(obj_dict.copy())\n",
    "            room_list.append(obj_list.copy())\n",
    "            obj_list.clear()\n",
    "                            \n",
    "        return room_list\n",
    "    \n",
    "    def save_dis(self):\n",
    "        torch.save(self.discriminator.state_dict(), \"save.pt\")\n",
    "\n",
    "def main():\n",
    "    import matplotlib.pyplot as plt\n",
    "    from time import time\n",
    "    import urllib, json\n",
    "    import torch\n",
    "\n",
    "    \n",
    "    batch_size = 16\n",
    "    epochs = 100\n",
    "    latent_dim = 16\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    \n",
    "    url = \"https://raw.githubusercontent.com/Chaoyuuu/Gather-Town-Datasets/master/datasets.json\"\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = json.loads(response.read())\n",
    "    input_data = torch.zeros(len(data)-(len(data)%batch_size), 17, 13, 10)\n",
    "\n",
    "    for i in range(len(input_data)):\n",
    "        for j in range(len(data[i][\"room\"])):\n",
    "            input_data[i][data[i][\"room\"][j][\"label\"]][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = 1\n",
    "            if data[i][\"room\"][j][\"orientation\"] == 0:\n",
    "                input_data[i][16][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = -1\n",
    "            elif data[i][\"room\"][j][\"orientation\"] == 1:\n",
    "                input_data[i][15][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = -1\n",
    "            elif data[i][\"room\"][j][\"orientation\"] == 2:\n",
    "                input_data[i][16][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = 1\n",
    "            else:\n",
    "                input_data[i][15][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = 1 \n",
    "    \n",
    "    \n",
    "    dataloader = DataLoader(input_data,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=1\n",
    "            )\n",
    "    noise_fn = lambda x: torch.randn((x, latent_dim), device=device)\n",
    "    gan = DCGAN(latent_dim, noise_fn, dataloader, device=device, batch_size=batch_size)\n",
    "    start = time()\n",
    "    for i in range(10):\n",
    "        print(f\"Epoch {i+1}; Elapsed time = {int(time() - start)}s\")\n",
    "        gan.train_epoch()\n",
    "        \n",
    "    result_tensor = gan.generate_samples()\n",
    "    result = gan.trans_samples(result_tensor)\n",
    "    print(len(result[0]))\n",
    "    print(result[0])\n",
    "    #print(result_tensor.size())\n",
    "    #pred_result = gan.discriminator(result_tensor)\n",
    "    #print(pred_result)\n",
    "    \n",
    "    #print(gan.discriminator(input_data[0]))\n",
    "    #gan.save_dis()\n",
    "    #print(len(result[0]))\n",
    "    #print(result[0])\n",
    "    #print(result)\n",
    "    #images = gan.generate_samples() * -1\n",
    "    #ims = tv.utils.make_grid(images, normalize=True)\n",
    "    #plt.imshow(ims.numpy().transpose((1,2,0)))\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5030]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "        self._init_modules()\n",
    "\n",
    "    def _init_modules(self):\n",
    "        \"\"\"Initialize the modules.\"\"\"\n",
    "        self.conv1 = nn.Conv2d(\n",
    "                in_channels=17,\n",
    "                out_channels=68,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=2,\n",
    "                bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.dropout_2d = nn.Dropout2d(0.3)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "                in_channels=68,\n",
    "                out_channels=136,\n",
    "                kernel_size=2,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=False)\n",
    "        self.linear1 = nn.Linear(136*6*5, 1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map samples to confidence they are real [0, 1].\"\"\"\n",
    "        intermediate = self.conv1(input_tensor)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.dropout_2d(intermediate)\n",
    "\n",
    "        intermediate = self.conv2(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "        intermediate = self.dropout_2d(intermediate)\n",
    "\n",
    "        intermediate = intermediate.view((-1, 136*6*5))\n",
    "        intermediate = self.linear1(intermediate)\n",
    "        output_tensor = self.sigmoid(intermediate)\n",
    "        \n",
    "        return output_tensor\n",
    "    \n",
    "def main():\n",
    "    my_discr = Discriminator()\n",
    "    my_discr.load_state_dict(torch.load(\"save.pt\"))\n",
    "    zeros = torch.zeros((1, 17, 13, 10))\n",
    "    my_discr.eval()\n",
    "    \n",
    "    print(my_discr.forward(zeros))\n",
    "     \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tv.transforms.Compose([\n",
    "            tv.transforms.Grayscale(num_output_channels=1),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5,), (0.5,))\n",
    "            ])\n",
    "    dataset = ImageFolder(\n",
    "            root=os.path.join(\"data\", \"mnist_png\", \"training\"),\n",
    "            transform=transform\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/r09944062/dcgan\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.abspath(\"dcgan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision as tv\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, batchnorm=True):\n",
    "        \"\"\"A generator for mapping a latent space to a sample space.\n",
    "        The sample space for this generator is single-channel, 28x28 images\n",
    "        with pixel intensity ranging from -1 to +1.\n",
    "        Args:\n",
    "            latent_dim (int): latent dimension (\"noise vector\")\n",
    "            batchnorm (bool): Whether or not to use batch normalization\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batchnorm = batchnorm\n",
    "        self._init_modules()\n",
    "\n",
    "    def _init_modules(self):\n",
    "        \"\"\"Initialize the modules.\"\"\"\n",
    "        # Project the input\n",
    "        self.linear1 = nn.Linear(self.latent_dim, 288*6*5, bias=False)\n",
    "        self.bn1d1 = nn.BatchNorm1d(288*3*3) if self.batchnorm else None\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "        # Convolutions\n",
    "        self.conv1 = nn.Conv2d(\n",
    "                in_channels=288,\n",
    "                out_channels=144,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False)\n",
    "        #6X5\n",
    "        self.bn2d1 = nn.BatchNorm2d(144) if self.batchnorm else None\n",
    "\n",
    "        self.conv2 = nn.ConvTranspose2d(\n",
    "                in_channels=144,\n",
    "                out_channels=72,\n",
    "                kernel_size=2,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False)\n",
    "        #7X6\n",
    "        self.bn2d2 = nn.BatchNorm2d(72) if self.batchnorm else None\n",
    "\n",
    "        self.conv3 = nn.ConvTranspose2d(\n",
    "                in_channels=72,\n",
    "                out_channels=18,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=2,\n",
    "                bias=False)\n",
    "        #\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
    "        intermediate = self.linear1(input_tensor)\n",
    "        intermediate = self.bn1d1(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "\n",
    "        intermediate = intermediate.view((-1, 256, 7, 7))\n",
    "\n",
    "        intermediate = self.conv1(intermediate)\n",
    "        if self.batchnorm:\n",
    "            intermediate = self.bn2d1(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "\n",
    "        intermediate = self.conv2(intermediate)\n",
    "        print(intermediate.shape())\n",
    "        if self.batchnorm:\n",
    "            intermediate = self.bn2d2(intermediate)\n",
    "        intermediate = self.leaky_relu(intermediate)\n",
    "\n",
    "        intermediate = self.conv3(intermediate)\n",
    "        output_tensor = self.tanh(intermediate)\n",
    "        return output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2 8]\n",
      "  [3 4 5]]\n",
      "\n",
      " [[5 6 7]\n",
      "  [7 8 9]]]\n",
      "tensor([[[1., 2., 8.],\n",
      "         [3., 4., 5.]],\n",
      "\n",
      "        [[5., 6., 7.],\n",
      "         [7., 8., 9.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[5., 6., 7.],\n",
      "         [7., 8., 9.]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "array3d = np.array([[[1, 2, 8], [3, 4, 5]],[[5, 6, 7], [7, 8, 9]]])\n",
    "print(array3d)\n",
    "\n",
    "tensor = torch.Tensor(array3d)\n",
    "print(tensor)\n",
    "\n",
    "print(tensor.size())\n",
    "max_index = torch.argmax(tensor, dim=0)\n",
    "print(max_index.size())\n",
    "\n",
    "for i in range(tensor.size(dim = 1)):\n",
    "    for j in range(tensor.size(dim = 2)):\n",
    "        tensor[0][i][j]=1\n",
    "        \n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "tensor([[ 0., -1.,  0.,  0., -1.,  0.,  0.,  0., -1., -1.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1., -1., -1.,  0., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1.,  0.,  0., -1., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1., -1.,  0.,  0., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1., -1.,  0.,  0., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1.,  0.,  0., -1., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -1.,  0.,  0.,  0., -1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "[{'label': 0, 'x': 4, 'y': 0, 'orientation': 0}, {'label': 3, 'x': 1, 'y': 0, 'orientation': 0}, {'label': 5, 'x': 8, 'y': 0, 'orientation': 0}, {'label': 5, 'x': 9, 'y': 0, 'orientation': 0}, {'label': 5, 'x': 0, 'y': 10, 'orientation': 0}, {'label': 5, 'x': 0, 'y': 11, 'orientation': 0}, {'label': 6, 'x': 5, 'y': 3, 'orientation': 0}, {'label': 8, 'x': 3, 'y': 4, 'orientation': 0}, {'label': 8, 'x': 7, 'y': 4, 'orientation': 0}, {'label': 8, 'x': 3, 'y': 5, 'orientation': 0}, {'label': 8, 'x': 7, 'y': 5, 'orientation': 0}, {'label': 8, 'x': 3, 'y': 6, 'orientation': 0}, {'label': 8, 'x': 7, 'y': 6, 'orientation': 0}, {'label': 8, 'x': 3, 'y': 7, 'orientation': 0}, {'label': 8, 'x': 7, 'y': 7, 'orientation': 0}, {'label': 8, 'x': 3, 'y': 8, 'orientation': 0}, {'label': 8, 'x': 7, 'y': 8, 'orientation': 0}, {'label': 8, 'x': 3, 'y': 9, 'orientation': 0}, {'label': 8, 'x': 7, 'y': 9, 'orientation': 0}, {'label': 9, 'x': 4, 'y': 4, 'orientation': 0}, {'label': 9, 'x': 4, 'y': 7, 'orientation': 0}, {'label': 11, 'x': 6, 'y': 5, 'orientation': 0}, {'label': 11, 'x': 4, 'y': 6, 'orientation': 0}, {'label': 11, 'x': 6, 'y': 8, 'orientation': 0}, {'label': 12, 'x': 5, 'y': 4, 'orientation': 0}]\n"
     ]
    }
   ],
   "source": [
    "import urllib, json\n",
    "import torch\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Chaoyuuu/Gather-Town-Datasets/master/datasets.json\"\n",
    "\n",
    "response = urllib.request.urlopen(url)\n",
    "\n",
    "data = json.loads(response.read())\n",
    "\n",
    "print(len(data))\n",
    "input_data = torch.zeros((32, 17, 13, 10))\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    for j in range(len(data[i][\"room\"])):\n",
    "        input_data[i][data[i][\"room\"][j][\"label\"]][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = 1\n",
    "        if data[1][\"room\"][2][\"orientation\"] == 0:\n",
    "            input_data[i][16][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = -1\n",
    "        elif data[1][\"room\"][2][\"orientation\"] == 1:\n",
    "            input_data[i][15][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = -1\n",
    "        elif data[1][\"room\"][2][\"orientation\"] == 2:\n",
    "            input_data[i][16][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = 1\n",
    "        else:\n",
    "            input_data[i][15][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = 1\n",
    "\n",
    "\n",
    "print(input_data[0][16])\n",
    "#print(data[1][\"room\"][2][\"label\"])\n",
    "\n",
    "#input_data = tensor.zeros((batch_size, 17, 13, 10))\n",
    "obj_list = []\n",
    "obj_dict = {\"label\":0,\n",
    "             \"x\":0,\n",
    "             \"y\":0\n",
    "             \"orientation\":0}\n",
    "for i in range(input_data.size(dim=1)-2):\n",
    "    for j in range(input_data.size(dim=2)):\n",
    "        for k in range(input_data.size(dim=3)):\n",
    "            if input_data[0][i][j][k] == 1:\n",
    "                obj_dict[\"label\"] = i\n",
    "                obj_dict[\"y\"] = j\n",
    "                obj_dict[\"x\"] = k\n",
    "                if input_data[0][16][j][k] == -1:\n",
    "                    obj_dict[\"orientation\"] = 0\n",
    "                elif input_data[0][16][j][k] == 1:\n",
    "                    obj_dict[\"orientation\"] = 2\n",
    "                elif input_data[0][15][j][k] == -1:\n",
    "                    obj_dict[\"orientation\"] = 1\n",
    "                else:\n",
    "                    obj_dict[\"orientation\"] = 3\n",
    "                #tmp = obj_dict.copy()\n",
    "                obj_list.append(obj_dict.copy())\n",
    "                \n",
    "print(obj_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-dfb8a718c1ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"room\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"room\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"room\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"room\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"room\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"orientation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_data = torch.zeros((32, 17, 13, 10))\n",
    "\n",
    "for i in range(32):\n",
    "    for j in len(data[i][\"room\"]):\n",
    "        input_data[i][data[i][\"room\"][j][\"label\"]][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = 1\n",
    "        if data[1][\"room\"][2][\"orientation\"] == 0:\n",
    "            input_data[i][16][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = -1\n",
    "        elif data[1][\"room\"][2][\"orientation\"] == 1:\n",
    "            input_data[i][15][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = -1\n",
    "        elif data[1][\"room\"][2][\"orientation\"] == 2:\n",
    "            input_data[i][16][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = 1\n",
    "        else:\n",
    "            input_data[i][15][data[i][\"room\"][j][\"y\"]][data[i][\"room\"][j][\"x\"]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
